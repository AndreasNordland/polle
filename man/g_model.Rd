% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/g_models.R
\name{g_model}
\alias{g_model}
\alias{g_glm}
\alias{g_glmnet}
\alias{g_rf}
\alias{g_sl}
\title{Define a propensity model/g-model object constructor}
\usage{
g_glm(formula = ~., family = "binomial", model = FALSE, ...)

g_glmnet(formula = ~., family = "binomial", alpha = 1, s = "lambda.min", ...)

g_rf(
  formula = ~.,
  num.trees = c(500),
  mtry = NULL,
  cv_args = list(K = 5, rep = 1),
  ...
)

g_sl(
  formula = ~.,
  SL.library = c("SL.mean", "SL.glm"),
  family = binomial(),
  ...
)
}
\arguments{
\item{formula}{An object of class \link{formula} specifying the design matrix for
the propensity model/g-model. Use \code{\link[=get_history_names]{get_history_names()}} to se the available
variable names.}

\item{family}{A description of the error distribution and link function to
be used in the model.}

\item{...}{Additional arguments passed to \code{\link[=glm]{glm()}}, \link[glmnet:glmnet]{glmnet::glmnet},
\link[ranger:ranger]{ranger::ranger} or \link[SuperLearner:SuperLearner]{SuperLearner::SuperLearner}.}

\item{alpha}{(Only used by \code{g_glmnet}) The elasticnet mixing parameter
between 0 and 1. alpha equal to 1 is the lasso penalty, and alpha equal
to 0 the ridge penalty.}

\item{s}{(Only used by \code{g_glmnet}) Value(s) of the penalty parameter
lambda at which predictions are required, see \code{\link[glmnet:predict.glmnet]{glmnet::predict.glmnet()}}.}

\item{num.trees}{(Only used by \code{g_rf}) Number of trees.}

\item{mtry}{(Only used by \code{g_rf}) Number of variables to possibly split
at in each node.}

\item{cv_args}{(Only used by \code{g_rf}) Cross-validation parameters.
Only used if multiple hyper-parameters are given. \code{K} is the number
of folds and
\code{rep} is the number of replications.}

\item{SL.library}{(Only used by \code{g_sl}) Either a character vector of prediction algorithms or
a list containing character vectors, see \link[SuperLearner:SuperLearner]{SuperLearner::SuperLearner}.}
}
\value{
g-model object constructor function with arguments 'A'
(action vector), 'H' (history matrix) and 'action_set'.
}
\description{
Use \code{g_glm}, \code{g_glmnet}, \code{g_rf}, and \code{g_sl} to define
a propensity model/g-model object constructor.
The constructors are used as input for \code{\link[=policy_eval]{policy_eval()}} and \code{\link[=policy_learn]{policy_learn()}}.
}
\details{
\code{g_glm} is a constructor for a generalized linear model. Specifically,
it is a wrapper of \code{\link[=glm]{glm()}}.\cr
\code{g_glmnet} is a constructor for a generalized linear model via
penalized maximum likelihood. It is a wrapper of \code{\link[glmnet:glmnet]{glmnet::glmnet()}}.\cr
\code{g_rf} is a constructor for a random forest model. It is a wrapper of
\code{\link[ranger:ranger]{ranger::ranger()}}. When multiple hyper-parameters are given, the
model with the lowest cross-validation error is selected.
}
\examples{
library("polle")
### Single stage:
source(system.file("sim", "single_stage.R", package="polle"))
d <- sim_single_stage(5e2, seed=1)
pd <- policy_data(d, action="A", covariates=list("Z", "B", "L"), utility="U")
pd

# defining a g-model:
get_history_names(pd)
g_model <- g_glm(formula = ~Z)

# evaluating the static policy a=1 using inverse
# propensity weighting based on the given g-model:
pe <- policy_eval(type = "ipw",
            policy_data = pd,
            policy = policy_def(static_policy(1)),
            g_model = g_model)
pe
pe$g_functions

### Two stages:
source(system.file("sim", "two_stage.R", package="polle"))
d2 <- sim_two_stage(5e2, seed=1)
pd2 <- policy_data(d2,
                  action = c("A_1", "A_2"),
                  baseline = c("B"),
                  covariates = list(L = c("L_1", "L_2"),
                                    C = c("C_1", "C_2")),
                  utility = c("U_1", "U_2", "U_3"))
pd2

# evaluating the static policy a=1 using inverse propensity weighting
# based on a state glm model across all stages:
pe2 <- policy_eval(type = "ipw",
            policy_data = pd2,
            policy = policy_def(static_policy(1), reuse = TRUE),
            g_model = g_glm(formula = ~ B + C))
pe2
pe2$g_functions

# available full history variable names at each stage:
get_history_names(pd2, stage = 1)
get_history_names(pd2, stage = 2)

# evaluating the static policy a=1 using inverse propensity weighting
# based on a full history glm model for each stage:
pe2 <- policy_eval(type = "ipw",
            policy_data = pd2,
            policy = policy_def(static_policy(1), reuse = TRUE),
            g_model = list(g_glm(~ L_1 + B),
                           g_glm(~ A_1 + L_2 + B)),
            g_full_history = TRUE)
pe2
pe2$g_functions
}
