% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/policy_learn.R
\name{policy_learn}
\alias{policy_learn}
\alias{print.policy_object}
\alias{print.policy_learner}
\alias{get_policy}
\title{Create Policy Learner Object:}
\usage{
policy_learn(
  type = "rql",
  alpha = 0,
  L = 1,
  save_cross_fit_models = FALSE,
  future_args = NULL,
  qv_models = NULL,
  qv_full_history = FALSE,
  policy_vars = NULL,
  policy_full_history = FALSE,
  depth = 2,
  split.step = 1,
  min.node.size = 1,
  hybrid = FALSE,
  search.depth = 2
)

\method{print}{policy_object}(x)

\method{print}{policy_learner}(x)

get_policy(object)
}
\arguments{
\item{type}{Type of policy learner method:
\itemize{
\item{} \code{"rql"}: Realistic Quality/Q-learning.
\item{} \code{"rqvl"}: Realistic V-restricted (doubly robust) Q-learning.
\item{} \code{"ptl"}: Policy Tree Learning.
}}

\item{alpha}{Probability threshold for determining realistic actions.}

\item{L}{(only used if \code{type = "rqvl"} or \code{type = "ptl"}) Number of folds for
cross-fitting.}

\item{save_cross_fit_models}{If \code{TRUE}, the cross-fitted models will be saved.}

\item{future_args}{Arguments passed to \code{\link[future.apply:future_apply]{future.apply::future_apply()}}.}

\item{qv_models}{(only used if \code{type = "rqvl"}) V-restricted Q-models created
by \code{\link[=q_glm]{q_glm()}}, \code{\link[=q_rf]{q_rf()}}, \code{\link[=q_sl]{q_sl()}} or similar functions.}

\item{qv_full_history}{(only used if \code{type = "rqvl"}) If \code{TRUE}, the full
history is used to fit each QV-model. If FALSE, the single stage/
"Markov type" history is used to fit each QV-model.}

\item{policy_vars}{(only used if \code{type = "ptl"}) Character vector/string or
list of character vectors/strings. Variable names used to construct a
V-restricted policy tree. The names must be a subset of the history variable
names, see \code{\link[=get_history_names]{get_history_names()}}.}

\item{policy_full_history}{(only used if \code{type = "ptl"}) If \code{TRUE}, the full
history is parsed to policy_tree. If FALSE, the single stage/"Markov type"
history is parsed to policy_tree.}

\item{depth}{(only used if \code{type = "ptl"}) The depth of the fitted policy
tree, see \code{\link[=policy_tree]{policy_tree()}}.}

\item{split.step}{(only used if \code{type = "ptl"}) The number of possible splits
to consider when performing policy tree search, see \code{\link[=policy_tree]{policy_tree()}}.}

\item{min.node.size}{(only used if \code{type = "ptl"}) An integer indicating the
smallest terminal node size permitted, see \code{\link[=policy_tree]{policy_tree()}}.}

\item{hybrid}{(only used if \code{type = "ptl"}) If \code{TRUE}, \code{\link[=hybrid_policy_tree]{hybrid_policy_tree()}} is
used to fit a policy tree.}

\item{search.depth}{(only used if \code{type = "ptl"} and \code{hybrid = TRUE}) Depth to
look ahead when splitting.}
}
\value{
Function of inherited class \code{"policy_learner"}.
Evaluating the function returns an object of class \code{"policy_object"}.
}
\description{
\code{policy_learn} is used to specify a policy learning method (Q-learning,
V-restricted (doubly robust) Q-learning and V-restricted policy tree
learning). Evaluating the policy learner returns a policy object.
}
\examples{
library("polle")
### Two stages:
source(system.file("sim", "two_stage.R", package="polle"))
par0 <- c(gamma = 0.5, beta = 1)
d <- sim_two_stage(2e3, seed=1, par=par0)
pd <- policy_data(d,
                  action = c("A_1", "A_2"),
                  baseline = c("BB"),
                  covariates = list(L = c("L_1", "L_2"),
                                    C = c("C_1", "C_2")),
                  utility = c("U_1", "U_2", "U_3"))
pd

### Q-learning
# specifying the learner:
pl <- policy_learn(type = "rql")
pl

# the policy learner can be used directly:
po <- pl(pd, q_models = q_glm())
po
head(get_policy(po)(pd))

# or the policy learner can be evaluated:
pe <- policy_eval(policy_data = pd,
            policy_learn = pl,
            q_models = q_glm())
pe
pe$policy_object; rm(pl, po, pe)

### V-restricted (Doubly Robust) Q-learning

# specifying the learner:
pl <- policy_learn(type = "rqvl",
                   qv_models = q_glm(formula = ~ C))

# evaluating the learned policy
pe <- policy_eval(policy_data = pd,
            policy_learn = pl,
            q_models = q_glm(),
            g_models = g_glm())
pe
pe$policy_object
pe$policy_object$qv_functions$stage_1
head(get_policy(pe$policy_object)(pd)); rm(pl, pe)

### V-restricted Policy Tree Learning

# specifying the learner:
pl <- policy_learn(type = "ptl",
                   policy_vars = list(c("C_1", "BB"),
                                      c("L_1", "BB")),
                   policy_full_history = TRUE)

# evaluating the learned policy:
set.seed(1)
pe <- policy_eval(policy_data = pd,
                  policy_learn = pl,
                  q_models = q_glm(),
                  g_models = g_glm())
pe
pe$policy_object$ptl_objects
}
\seealso{
\code{\link[=policy_def]{policy_def()}}, \code{\link[=get_policy_functions]{get_policy_functions()}}.
}
