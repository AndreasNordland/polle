% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/policy_learn.R
\name{policy_learn}
\alias{policy_learn}
\title{Create Policy Learner Object:}
\usage{
policy_learn(
  type = "rql",
  alpha = 0,
  L = 1,
  save_cross_fit_models = FALSE,
  future_args = NULL,
  qv_models = NULL,
  qv_full_history = FALSE,
  policy_vars = NULL,
  policy_full_history = FALSE,
  depth = 2,
  split.step = 1,
  min.node.size = 1,
  hybrid = FALSE,
  search.depth = 2
)
}
\arguments{
\item{type}{Type of policy learner method:
\itemize{
\item{} "rql": Realistic Quality/Q-learning.
\item{} "rqvl": Realistic V-restricted (doubly robust) Q-learning.
\item{} "ptl": Policy Tree Learning.
}}

\item{alpha}{Probability threshold for determining realistic actions.}

\item{L}{(only used if type = "rqvl" or type = "ptl") Number of folds for cross-fitting.}

\item{save_cross_fit_models}{If TRUE, the cross-fitted models will be saved.}

\item{future_args}{Arguments passed to \code{\link[future.apply:future_apply]{future.apply::future_apply()}}.}

\item{qv_models}{(only used if type = "rqvl") V-restricted Q-models created by \code{\link[=q_glm]{q_glm()}}, \code{\link[=q_rf]{q_rf()}}, \code{\link[=q_sl]{q_sl()}} or similar functions.}

\item{qv_full_history}{(only used if type = "rqvl") If TRUE, the full history is used to fit each QV-model. If FALSE, the single stage/"Markov type" history is used to fit each QV-model.}

\item{policy_vars}{(only used if type = "ptl") Character vector/string or list of character vectors/strings. Variable names used to construct a V-restricted policy tree. The names must be a subset of the history variable names, see \code{\link[=get_history_names]{get_history_names()}}.}

\item{policy_full_history}{(only used if type = "ptl") If TRUE, the full history is parsed to policy_tree. If FALSE, the single stage/"Markov type" history is parsed to policy_tree.}

\item{depth}{(only used if type = "ptl") The depth of the fitted policy tree, see \code{\link[=policy_tree]{policy_tree()}}.}

\item{split.step}{(only used if type = "ptl") The number of possible splits to consider when performing policy tree search, see \code{\link[=policy_tree]{policy_tree()}}.}

\item{min.node.size}{(only used if type = "ptl") An integer indicating the smallest terminal node size permitted, see \code{\link[=policy_tree]{policy_tree()}}.}

\item{hybrid}{(only used if type = "ptl") If TRUE, \code{\link[=hybrid_policy_tree]{hybrid_policy_tree()}} is used to fit a policy tree.}

\item{search.depth}{(only used if type = "ptl" and hybrid = TRUE) Depth to look ahead when splitting.}
}
\description{
\code{policy_learn} is used to specify a policy learning method (Q-learning, V-restricted (doubly robust) Q-learning and V-restricted policy tree learning).
}
\details{
rqvl: a model is  fitted for each model in the action set
}
\section{Value}{

Returns a function of class "policy_function".
}

\examples{
library("polle")
### Simulating two-stage policy data
source(system.file("sim", "two_stage.R", package="polle"))
par0 <- c(gamma = 0.5, beta = 1)
d <- sim_two_stage(2e3, seed=1, par=par0)
pd <- policy_data(d,
                  action = c("A_1", "A_2"),
                  covariates = list(L = c("L_1", "L_2"),
                                    C = c("C_1", "C_2")),
                  utility = c("U_1", "U_2", "U_3"))
pd

### Q-learning
# specifying the learner:
pl <- policy_learn(type = "rql")
pl

# the policy learner can be used directly:
po <- pl(pd, q_models = q_glm())
po
head(get_policy(po)(pd))

# or the policy learner can be evaluated:
pe <- policy_eval(policy_data = pd,
            policy_learn = pl,
            q_models = q_glm())
pe
pe$policy_object; rm(pl, po, pe)

### V-restricted (Doubly Robust) Q-learning

# specifying the learner:
pl <- policy_learn(type = "rqvl",
                   qv_models = q_glm(formula = ~ C))

# evaluating the learned policy
pe <- policy_eval(policy_data = pd,
            policy_learn = pl,
            q_models = q_glm(),
            g_models = g_glm())
pe
pe$policy_object
pe$policy_object$qv_functions$stage_1
head(get_policy(pe$policy_object)(pd)); rm(pl, pe)

### V-restricted Policy Tree Learning

# specifying the learner:
pl <- policy_learn(type = "ptl", policy_vars = c("C"))

# evaluating the learned policy:
pe <- policy_eval(policy_data = pd,
                  policy_learn = pl,
                  q_models = q_glm(),
                  g_models = g_glm())
pe
pe$policy_object$ptl_objects

### Cross-fitted Policy Learning
}
