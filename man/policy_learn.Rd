% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/policy_learn.R
\name{policy_learn}
\alias{policy_learn}
\alias{policy_object}
\alias{print.policy_object}
\alias{print.policy_learn}
\title{Create Policy Learner}
\usage{
policy_learn(
  type = "rql",
  alpha = 0,
  L = 1,
  save_cross_fit_models = FALSE,
  future_args = list(future.seed = TRUE),
  full_history = FALSE,
  rqvl_args = list(qv_models = NULL),
  ptl_args = list(policy_vars = NULL, depth = 2, split.step = 1, min.node.size = 1,
    hybrid = FALSE, search.depth = 2),
  bowl_args = list(policy_vars = NULL, reuse_scales = TRUE, res.lasso = TRUE, loss =
    "hinge", kernel = "linear", augment = FALSE, c = 2^(-2:2), sigma = c(0.03, 0.05,
    0.07), s = 2^(-2:2), m = 4),
  earl_args = list(moPropen = NULL, moMain = NULL, moCont = NULL, regime = NULL, iter =
    0L, fSet = NULL, lambdas = 0.5, cvFolds = 0L, surrogate = "hinge", kernel = "linear",
    kparam = NULL, verbose = 0L),
  rwl_args = list(moPropen = NULL, moMain = NULL, regime = NULL, fSet = NULL, lambdas =
    2, cvFolds = 0L, kernel = "linear", kparam = NULL, responseType = "continuous",
    verbose = 0L)
)

\method{print}{policy_object}(x, ...)

\method{print}{policy_learn}(x, ...)
}
\arguments{
\item{type}{Type of policy learner method:
\itemize{
\item{} \code{"rql"}: Realistic Quality/Q-learning.
\item{} \code{"rqvl"}: Realistic V-restricted (doubly robust) Q-learning.
\item{} \code{"ptl"}: Policy Tree Learning.
\item{} \code{"bowl"}: Backwards Outcome Weighted Learning.
\item{} \code{"earl"}: Efficient Augmentation and Relaxation Learning (only single stage).
\item{} \code{"rwl"}: Residual Weighted Learning (only single stage).
}}

\item{alpha}{Probability threshold for determining realistic actions.}

\item{L}{(only used if \code{type = "rqvl"} or \code{type = "ptl"}) Number of folds for
cross-fitting.}

\item{save_cross_fit_models}{If \code{TRUE}, the cross-fitted models will be saved.}

\item{future_args}{Arguments passed to \code{\link[future.apply:future_apply]{future.apply::future_apply()}}.}

\item{full_history}{If \code{TRUE}, the full
history is used to fit each policy function (e.g. QV-model, policy tree). If FALSE, the single stage/
"Markov type" history is used to fit each policy function.}

\item{rqvl_args}{Arguments used if \code{type = "rqvl"}.
\itemize{
\item{} \code{qv_models}: V-restricted Q-models created by \code{\link[=q_glm]{q_glm()}}, \code{\link[=q_rf]{q_rf()}}, \code{\link[=q_sl]{q_sl()}} or similar functions.
}}

\item{ptl_args}{Arguments used if \code{type = "ptl"}, see \code{\link[policytree:policy_tree]{policytree::policy_tree()}}.
\itemize{
\item{} \code{policy_vars}: Character vector/string or
list of character vectors/strings. Variable names used to construct a
V-restricted policy tree. The names must be a subset of the history variable
names, see \code{\link[=get_history_names]{get_history_names()}}.
\item{} \code{depth}: Numeric or numeric vector.
The depth of the fitted policy tree for each stage.
\item{} \code{split.step}:  Numeric or numeric vector.
The number of possible splits to consider when performing policy tree search
at each stage.
\item{} \code{min.node.size}: Numeric or numeric vector.
The smallest terminal node size permitted at each stage.
\item{} \code{hybrid}: If \code{TRUE}, \code{\link[policytree:hybrid_policy_tree]{policytree::hybrid_policy_tree()}} is used to fit a policy tree.
\item{} \code{search.depth}: (only used if \code{hybrid = TRUE})
Numeric or numeric vector. Depth to look ahead when splitting at each stage.
}}

\item{bowl_args}{Arguments used if \code{type = "bowl"}, see \code{\link[DTRlearn2:owl]{DTRlearn2::owl()}}.
\itemize{
\item{} \code{policy_vars}: Character vector/string or
list of character vectors/strings. Variable names used to construct a
V-restricted policy tree. The names must be a subset of the history variable
names, see \code{\link[=get_history_names]{get_history_names()}}.
\item{} \code{reuse_scales}: If \code{TRUE}, the scales of the history matrix will be
saved and reused when applied to (new) test data.
\item{} \code{res.lasso}: If \code{TRUE} a lasso penalty is applied.
\item{} \code{loss}: Loss function.
\item{} \code{kernel}: Type of kernel used by the support vector machine.
\item{} \code{augment}: If \code{TRUE} the
outcomes are augmented at each stage.
\item{} \code{c}: Regularization parameter.
\item{} \code{sigma}: Tuning parameter.
\item{} \code{s}: Slope parameter.
\item{} \code{m}: Number of folds for cross-validation of tuning parameters.
}}

\item{earl_args}{Arguments used if \code{type = "earl"}, see \code{\link[DynTxRegime:earl]{DynTxRegime::earl()}}.
\itemize{
\item{} \code{moPropen} Propensity model of class "ModelObj", see \link[modelObj:modelObj]{modelObj::modelObj}.
\item{} \code{moMain} Main effects outcome model of class "ModelObj".
\item{} \code{moCont} Contrast outcome model of class "ModelObj".
\item{} \code{regime} An object of class \link{formula} specifying the design
of the policy/regime.
\item{} ... see \code{\link[DynTxRegime:earl]{DynTxRegime::earl()}} for additional arguments.
}}

\item{rwl_args}{Arguments used if \code{type = "rwl"}, see \code{\link[DynTxRegime:rwl]{DynTxRegime::rwl()}}.
\itemize{
\item{} \code{moPropen} Propensity model of class "ModelObj", see \link[modelObj:modelObj]{modelObj::modelObj}.
\item{} \code{moMain} Main effects outcome model of class "ModelObj".
\item{} \code{regime} An object of class \link{formula} specifying the design
of the policy/regime.
\item{} ... see \code{\link[DynTxRegime:rwl]{DynTxRegime::rwl()}} for additional arguments.
}}

\item{x}{Object of class "policy_object" or "policy_learn".}

\item{...}{Additional arguments passed to print.}
}
\value{
Function of inherited class \code{"policy_learn"}.
Evaluating the function on a \link{policy_data} object returns an object of
class \link{policy_object}. A policy object is a list containing all or
some of the following elements:
\item{\code{q_functions}}{Fitted Q-functions. Object of class "nuisance_functions".}
\item{\code{g_functions}}{Fitted g-functions. Object of class "nuisance_functions".}
\item{\code{action_set}}{Sorted character vector describing the action set, i.e.,
the possible actions at each stage.}
\item{\code{alpha}}{Numeric. Probability threshold to determine realistic actions.}
\item{\code{K}}{Integer. Maximal number of stages.}
\item{\code{qv_functions}}{(only if \code{type = "rqvl"}) Fitted V-restricted
Q-functions. Contains a fitted model for each stage and action.}
\item{\code{ptl_objects}}{(only if \code{type = "ptl"}) Fitted V-restricted
policy trees. Contains a \link{policy_tree} for each stage.}
\item{\code{ptl_designs}}{(only if \code{type = "ptl"}) Specification of the
V-restricted design matrix for each stage}
}
\description{
\code{policy_learn()} is used to specify a policy learning method (Q-learning,
V-restricted (doubly robust) Q-learning, V-restricted policy tree
learning and outcome weighted learning). Evaluating the policy learner returns a policy object.
}
\details{
For references on V-restricted Q-learning (\code{type = "rqvl"}), see \doi{10.1515/ijb-2015-0052}.\cr
For references on policy tree learning (\code{type = "ptl"}), see \doi{10.48550/arXiv.1810.04778}.\cr
For references on (augmented) outcome weighted learning, see \doi{10.1002/sim.7844}.
}
\section{S3 generics}{

The following S3 generic functions are available for an object of
class "policy_object":
\itemize{
\item{\code{\link[=get_g_functions]{get_g_functions()}}}{ Extract the fitted g-functions.}
\item{\code{\link[=get_q_functions]{get_q_functions()}}}{ Extract the fitted Q-functions.}
\item{\code{\link[=get_policy]{get_policy()}}}{ Extract the fitted policy object.}
\item{\code{\link[=get_policy_functions]{get_policy_functions()}}}{ Extract the fitted policy function for
a given stage.}
\item{\code{\link[=get_policy_actions]{get_policy_actions()}}}{ Extract the (fitted) policy actions.}
}
}

\examples{
library("polle")
### Two stages:
source(system.file("sim", "two_stage.R", package="polle"))
d <- sim_two_stage(5e2, seed=1)
pd <- policy_data(d,
                  action = c("A_1", "A_2"),
                  baseline = c("BB"),
                  covariates = list(L = c("L_1", "L_2"),
                                    C = c("C_1", "C_2")),
                  utility = c("U_1", "U_2", "U_3"))
pd

### V-restricted (Doubly Robust) Q-learning

# specifying the learner:
pl <- policy_learn(type = "rqvl",
                   rqvl_args = list(qv_models = list(q_glm(formula = ~ C_1 + BB),
                                                     q_glm(formula = ~ L_1 + BB))),
                   full_history = TRUE)

# evaluating the learned policy
pe <- policy_eval(policy_data = pd,
                  policy_learn = pl,
                  q_models = q_glm(),
                  g_models = g_glm())
pe
# getting the policy object:
po <- get_policy_object(pe)
# inspecting the fitted QV-model for each action strata at stage 1:
po$qv_functions$stage_1
head(get_policy(pe)(pd))
}
\seealso{
\code{\link[=policy_eval]{policy_eval()}}
}
