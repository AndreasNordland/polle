% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/policy_eval.R
\name{policy_eval}
\alias{policy_eval}
\alias{coef.policy_eval}
\alias{IC.policy_eval}
\alias{vcov.policy_eval}
\alias{print.policy_eval}
\alias{summary.policy_eval}
\alias{estimate.policy_eval}
\alias{merge.policy_eval}
\alias{+.policy_eval}
\title{Policy Evaluation}
\usage{
policy_eval(
  policy_data,
  policy = NULL,
  policy_learn = NULL,
  g_functions = NULL,
  g_models = g_glm(),
  g_full_history = FALSE,
  q_functions = NULL,
  q_models = q_glm(),
  q_full_history = FALSE,
  type = "dr",
  M = 1,
  future_args = list(future.seed = TRUE)
)

\method{coef}{policy_eval}(object, ...)

\method{IC}{policy_eval}(x, ...)

\method{vcov}{policy_eval}(object, ...)

\method{print}{policy_eval}(x, ...)

\method{summary}{policy_eval}(object, ...)

\method{estimate}{policy_eval}(x, ..., labels = x$name)

\method{merge}{policy_eval}(x, y, ..., paired = TRUE)

\method{+}{policy_eval}(x, ...)
}
\arguments{
\item{policy_data}{Policy data object created by \code{\link[=policy_data]{policy_data()}}.}

\item{policy}{Policy object created by \code{\link[=policy_def]{policy_def()}}.}

\item{policy_learn}{Policy learner object created by \code{\link[=policy_learn]{policy_learn()}}.}

\item{g_functions}{Fitted g-model objects, see \code{\link[=fit_g_functions]{fit_g_functions()}}.
Preferably, use \code{g_models}.}

\item{g_models}{List of action probability models/g-models for each stage
created by \code{\link[=g_glm]{g_glm()}}, \code{\link[=g_rf]{g_rf()}}, \code{\link[=g_sl]{g_sl()}} or similar functions.
Only used for evaluation if \code{g_functions} is \code{NULL}.
If a single model is provided and \code{g_full_history} is \code{FALSE},
a single g-model is fitted across all stages. If \code{g_full_history} is
\code{TRUE} the model is reused at every stage.}

\item{g_full_history}{If TRUE, the full history is used to fit each g-model.
If FALSE, the state/Markov type history is used to fit each g-model.}

\item{q_functions}{Fitted Q-model objects, see \code{\link[=fit_Q_functions]{fit_Q_functions()}}.
Only valid if the Q-functions are fitted using the same policy.
Preferably, use \code{q_models}.}

\item{q_models}{Outcome regression models/Q-models created by
\code{\link[=q_glm]{q_glm()}}, \code{\link[=q_rf]{q_rf()}}, \code{\link[=q_sl]{q_sl()}} or similar functions.
Only used for evaluation if \code{q_functions} is \code{NULL}.
If a single model is provided, the model is reused at every stage.}

\item{q_full_history}{Similar to g_full_history.}

\item{type}{Type of evaluation (dr/doubly robust, ipw/inverse propensity weighting, or/outcome regression).}

\item{M}{Number of folds for cross-fitting.}

\item{future_args}{Arguments passed to \code{\link[future.apply:future_apply]{future.apply::future_apply()}}.}

\item{object, x, y}{Objects of class "policy_eval".}

\item{...}{Additional arguments.}

\item{labels}{Name(s) of the estimate(s).}

\item{paired}{\code{TRUE} indicates that the estimates are based on
the same data sample.}
}
\value{
\code{policy_eval()} returns an object of class "policy_eval".
The object is a list containing the following elements:
\item{\code{value_estimate}}{Numeric. The estimated value of the policy.}
\item{\code{type}}{Character string. The type of evaluation ("dr", "ipw", "or").}
\item{\code{IC}}{Numeric vector. Estimated influence curve associated with the value estimate.}
\item{\code{value_estimate_ipw}}{(only if \code{type = "dr"}) Numeric.
The estimated value of the policy based on inverse probability weighting.}
\item{\code{value_estimate_or}}{(only if \code{type = "dr"}) Numeric.
The estimated value of the policy based on outcome regression.}
\item{\code{id}}{Character vector. The IDs of the observations.}
\item{\code{policy_actions}}{\link{data.table} with keys id and stage. Actions
associated with the policy for every observation and stage.}
\item{\code{policy_object}}{(only if \code{policy = NULL} and \code{M = 1})
The policy object returned by \code{policy_learn}, see \link{policy_learn}.}
\item{\code{g_functions}}{(only if \code{M = 1}) The
fitted g-functions. Object of class "nuisance_functions".}
\item{\code{q_functions}}{(only if \code{M = 1}) The
fitted Q-functions. Object of class "nuisance_functions".}
\item{\code{cross_fits}}{(only if \code{M > 1}) List containing the
"policy_eval" object for every (validation) fold.}
\item{\code{folds}}{(only if \code{M > 1}) The (validation) folds used
for cross-fitting.}
}
\description{
\code{policy_eval()} is used to estimate the value of a given fixed policy or a data adaptive policy (e.g. a policy learned from the data).
}
\section{S3 generics}{

The following S3 generic functions are available for an object of
class \code{policy_data}:
\itemize{
\item{\code{\link[=get_g_functions]{get_g_functions()}}}{ Extract the fitted g-functions.}
\item{\code{\link[=get_q_functions]{get_q_functions()}}}{ Extract the fitted Q-functions.}
\item{\code{\link[=get_policy]{get_policy()}}}{ Extract the fitted policy object.}
\item{\code{\link[=get_policy_functions]{get_policy_functions()}}}{ Extract the fitted policy function for
a given stage.}
\item{\code{\link[=get_policy_actions]{get_policy_actions()}}}{ Extract the (fitted) policy actions.}
}
}

\examples{
library("polle")
### Single stage:
source(system.file("sim", "single_stage.R", package="polle"))
d1 <- sim_single_stage(5e2, seed=1)
pd1 <- policy_data(d1, action="A", covariates=list("Z", "B", "L"), utility="U")
pd1

# defining a static policy:
pl1 <- policy_def(static_policy(1))

# evaluating the policy:
pe1 <- policy_eval(policy_data = pd1,
                   policy = pl1,
                   g_models = g_glm(),
                   q_models = q_glm())

# summarizing the estimated value of the policy:
# (equivalent to summary(pe1)):
pe1
coef(pe1) # value coefficient
sqrt(vcov(pe1)) # value standard error

# getting the g-function and Q-function values:
head(predict(get_g_functions(pe1), pd1))
head(predict(get_q_functions(pe1), pd1))

# getting the fitted influence curve (IC) for the value:
head(IC(pe1))

# evaluating the policy using random forest nuisance models:
set.seed(1)
pe1_rf <- policy_eval(policy_data = pd1,
                   policy = pl1,
                   g_models = g_rf(),
                   q_models = q_rf())

# merging the two estimates (equivalent to pe1 + pe1_rf):
(est1 <- merge(pe1, pe1_rf))
coef(est1)
head(IC(est1))

### Two stages:
source(system.file("sim", "two_stage.R", package="polle"))
d2 <- sim_two_stage(5e2, seed=1)
pd2 <- policy_data(d2,
                  action = c("A_1", "A_2"),
                  covariates = list(L = c("L_1", "L_2"),
                                    C = c("C_1", "C_2")),
                  utility = c("U_1", "U_2", "U_3"))
pd2

# defining a policy learner based on cross-fitted doubly robust Q-learning:
pl2 <- policy_learn(type = "rqvl",
                   qv_models = list(q_glm(~C_1), q_glm(~C_1+C_2)),
                   full_history = TRUE,
                   L = 2) # number of folds for cross-fitting

# evaluating the policy learner using 2-fold cross fitting:
pe2 <- policy_eval(type = "dr",
                   policy_data = pd2,
                   policy_learn = pl2,
                   q_models = q_glm(),
                   g_models = g_glm(),
                   M = 2) # number of folds for cross-fitting
# summarizing the estimated value of the policy:
pe2

# getting the cross-fitted policy actions:
head(get_policy_actions(pe2))
}
\seealso{
\link[lava:IC]{lava::IC}, \link[lava:estimate.default]{lava::estimate.default}.
}
