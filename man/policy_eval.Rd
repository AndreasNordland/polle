% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/policy_eval.R
\name{policy_eval}
\alias{policy_eval}
\alias{coef.policy_eval}
\alias{IC.policy_eval}
\alias{vcov.policy_eval}
\alias{summary.policy_eval}
\alias{estimate.policy_eval}
\title{Policy Evaluation}
\usage{
policy_eval(
  policy_data,
  policy = NULL,
  policy_learn = NULL,
  g_functions = NULL,
  g_models = g_glm(),
  g_full_history = FALSE,
  q_functions = NULL,
  q_models = q_glm(),
  q_full_history = FALSE,
  type = "dr",
  M = 1,
  future_args = list(future.seed = TRUE)
)

\method{coef}{policy_eval}(object, ...)

\method{IC}{policy_eval}(x, ...)

\method{vcov}{policy_eval}(object, ...)

\method{summary}{policy_eval}(object, ...)

\method{estimate}{policy_eval}(x, ..., labels = x$name)
}
\arguments{
\item{policy_data}{Policy data object created by \code{\link[=policy_data]{policy_data()}}.}

\item{policy}{Policy object created by \code{\link[=policy_def]{policy_def()}}.}

\item{policy_learn}{Policy learner object created by \code{\link[=policy_learn]{policy_learn()}}.}

\item{g_functions}{Fitted g-model objects, see \code{\link[=fit_g_functions]{fit_g_functions()}}. Preferably, use \code{g_models}.}

\item{g_models}{Propensity models/g-models created by \code{\link[=g_glm]{g_glm()}}, \code{\link[=g_rf]{g_rf()}}, \code{\link[=g_sl]{g_sl()}} or similar functions. Only used for evaluation if \code{g_functions} is NULL.}

\item{g_full_history}{If TRUE, the full history is used to fit each g-model. If FALSE, the state/Markov type history is used to fit each g-model.}

\item{q_functions}{Fitted Q-model objects, see \code{\link[=fit_Q_functions]{fit_Q_functions()}}. Only valid if the Q-functions are fitted using the same policy. Preferably, use \code{q_models}.}

\item{q_models}{Outcome regression models/Q-models created by \code{\link[=q_glm]{q_glm()}}, \code{\link[=q_rf]{q_rf()}}, \code{\link[=q_sl]{q_sl()}} or similar functions. Only used for evaluation if \code{q_functions} is NULL.}

\item{q_full_history}{Similar to g_full_history.}

\item{type}{Type of evaluation (dr/doubly robust, ipw/inverse propensity weighting, or/outcome regression).}

\item{M}{Number of folds for cross-fitting.}

\item{future_args}{Arguments passed to \code{\link[future.apply:future_apply]{future.apply::future_apply()}}.}
}
\value{
\code{policy_eval()} returns an object of class "policy_eval".
The object is a list containing the following elements:
\item{\code{value_estimate}}{Numeric. The estimated value of the policy.}
\item{\code{type}}{Character string. The type of evaluation ("dr", "ipw", "or").}
\item{\code{IC}}{Numeric vector. Estimated influence curve associated with the value estimate.}
\item{\code{value_estimate_ipw}}{(only if \code{type = "dr"}) Numeric.
The estimated value of the policy based on inverse probability weighting.}
\item{\code{value_estimate_or}}{(only if \code{type = "dr"}) Numeric.
The estimated value of the policy based on outcome regression.}
\item{\code{id}}{Character vector. The IDs of the observations.}
\item{\code{policy_actions}}{\link{data.table} with keys id and stage. Actions
associated with the policy for every observation and stage.}
\item{\code{policy_object}}{(only if \code{policy = NULL} and \code{M = 1})
The policy object returned by \code{policy_learn}, see \link{policy_learn}.}
\item{\code{g_functions}}{(only if \code{M = 1}) The
fitted g-functions. Object of class "nuisance_functions".}
\item{\code{q_functions}}{(only if \code{M = 1}) The
fitted Q-functions. Object of class "nuisance_functions".}
\item{\code{cross_fits}}{(only if \code{M > 1}) List containing the
"policy_eval" object for every (validation) fold.}
\item{\code{folds}}{(only if \code{M > 1}) The (validation) folds used
for cross-fitting.}
}
\description{
\code{policy_eval} is used to estimate the value of a given fixed policy or a data adaptive policy (e.g. a policy learned from the data).
}
\examples{
library("polle")
### Single stage:
source(system.file("sim", "single_stage.R", package="polle"))
d1 <- sim_single_stage(5e2, seed=1)
pd1 <- policy_data(d1, action="A", covariates=list("Z", "B", "L"), utility="U")
pd1
# defining a static policy:
pl1 <- policy_def(static_policy(1))
# evaluating the policy:
pe1 <- policy_eval(policy_data = pd1,
            policy = pl1,
            g_models = g_glm(),
            q_models = q_glm())
# summarising the estimated value of the policy:
pe1

### Two stages:
source(system.file("sim", "two_stage.R", package="polle"))
d2 <- sim_two_stage(5e2, seed=1)
pd2 <- policy_data(d2,
                  action = c("A_1", "A_2"),
                  covariates = list(L = c("L_1", "L_2"),
                                    C = c("C_1", "C_2")),
                  utility = c("U_1", "U_2", "U_3"))
pd2
# defining a policy learner based on cross-fitted doubly robust Q-learning:
pl2 <- policy_learn(type = "rqvl",
                   qv_models = list(q_glm(~C_1), q_glm(~C_1+C_2)),
                   full_history = TRUE,
                   L = 2) # number of folds for cross-fitting
pe2 <- policy_eval(type = "dr",
                   policy_data = pd2,
                   policy_learn = pl2,
                   q_models = q_glm(),
                   g_models = g_glm(),
                   M = 2) # number of folds for cross-evaluation
pe2
# getting the influence curve for the value:
head(IC(pe2))
}
