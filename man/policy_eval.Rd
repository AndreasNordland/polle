% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/policy_eval.R
\name{policy_eval}
\alias{policy_eval}
\title{Policy Evaluation}
\usage{
policy_eval(
  policy_data,
  policy = NULL,
  policy_learn = NULL,
  g_functions = NULL,
  g_models = g_glm(),
  g_full_history = FALSE,
  q_functions = NULL,
  q_models = q_glm(),
  q_full_history = FALSE,
  type = "dr",
  cross_fit = FALSE,
  M = 5,
  future_args = list(future.seed = TRUE)
)
}
\arguments{
\item{policy_data}{Policy data object created by \code{\link[=policy_data]{policy_data()}}.}

\item{policy}{Policy object created by \code{\link[=policy_def]{policy_def()}}.}

\item{policy_learn}{Policy learner object created by \code{\link[=policy_learn]{policy_learn()}}.}

\item{g_functions}{Fitted g-model objects.}

\item{g_models}{Propensity models/g-models created by \code{\link[=g_glm]{g_glm()}}, \code{\link[=g_rf]{g_rf()}}, \code{\link[=g_sl]{g_sl()}} or similar functions.}

\item{g_full_history}{If TRUE, the full history is used to fit each g-model. If FALSE, the single stage/"Markov type" history is used to fit each g-model.}

\item{q_functions}{Fitted Q-model objects.}

\item{q_models}{Outcome regression models/Q-models created by \code{\link[=q_glm]{q_glm()}}, \code{\link[=q_rf]{q_rf()}}, \code{\link[=q_sl]{q_sl()}} or similar functions.}

\item{q_full_history}{Similar to g_full_history.}

\item{type}{Type of evaluation (dr/doubly robust, ipw/inverse propensity weighting, or/outcome regression).}

\item{cross_fit}{If TRUE, the evaluation will be cross-fitted.}

\item{M}{Number of folds for the cross-fitting.}

\item{future_args}{Arguments passed to \code{\link[future.apply:future_apply]{future.apply::future_apply()}}.}
}
\description{
\code{policy_eval} is used to estimate the value of a given fixed policy or a data adaptive policy (e.g. policy learned from the data).
}
