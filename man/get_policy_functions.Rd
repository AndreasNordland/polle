% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/policy_learn.R, R/ptl.R, R/rqvl.R
\name{get_policy_functions}
\alias{get_policy_functions}
\alias{get_policy_functions.PTL}
\alias{get_policy_functions.RQVL}
\title{Get Policy Functions}
\usage{
get_policy_functions(object, stage)

\method{get_policy_functions}{PTL}(object, stage)

\method{get_policy_functions}{RQVL}(object, stage)
}
\arguments{
\item{object}{Object of class "policy_object" or "policy_eval",
see \link{policy_learn} and \link{policy_eval}.}

\item{stage}{Integer. Stage number.}
}
\value{
Functions with arguments:
\item{\code{H}}{\link{data.table} containing the variables needed to evaluate the policy (and g-function).}
}
\description{
\code{get_policy_functions()} returns a function defining the policy at
the given stage. \code{get_policy_functions()} is useful when implementing
the learned policy.
}
\examples{
library("polle")
### Two stages:
source(system.file("sim", "two_stage.R", package="polle"))
d <- sim_two_stage(5e2, seed=1)
pd <- policy_data(d,
                  action = c("A_1", "A_2"),
                  baseline = "BB",
                  covariates = list(L = c("L_1", "L_2"),
                                    C = c("C_1", "C_2")),
                  utility = c("U_1", "U_2", "U_3"))
pd

### Realistic V-restricted (Doubly Robust) Q-learning
# specifying the learner:
pl <- policy_learn(type = "rqvl",
                   full_history = TRUE,
                   qv_models = list(q_glm(formula = ~ C_1),
                                    q_glm(formula = ~ L_2)),
                   alpha = 0.05)
# applying the learner:
po <- pl(policy_data = pd,
         q_models = q_glm(),
         g_models = g_glm())
po

# getting the policy function at stage 2:
pf2 <- get_policy_functions(po, stage = 2)
args(pf2)

# applying the policy function to new data:
set.seed(1)
L_2 <- rnorm(n = 10)
new_H <- data.table(BB = "group1",
                    C = rnorm(n = 10),
                    L = L_2,
                    L_2 = L_2)
d2 <- pf2(H = new_H)
d2

# comparing get_policy_functions() and get_policy() when
# used on an object of class "policy_object":
new_H <- get_history(pd, stage = 2, full_history = TRUE)$H
new_H$L <- new_H$L_2
new_H$C <- new_H$C_2
all.equal(
 unname(pf2(H = new_H)),
 get_policy(po)(pd)[stage == 2]$d
)
rm(pl, po, d2, pf2, new_H, L_2)

### Realistic V-restricted Policy Tree Learning
# specifying the learner:
pl <- policy_learn(type = "ptl",
                   policy_vars = list(c("C_1", "BB"),
                                      c("L_1", "BB")),
                   full_history = TRUE,
                   alpha = 0.05)

# evaluating the learner:
pe <- policy_eval(policy_data = pd,
                  policy_learn = pl,
                  q_models = q_glm(),
                  g_models = g_glm())

# getting the policy function at stage 2:
pf2 <- get_policy_functions(pe, stage = 2)
args(pf2)

# applying the policy function to new data:
set.seed(1)
L_1 <- rnorm(n = 10)
new_H <- data.table(C = rnorm(n = 10),
                    L = L_1,
                    L_1 = L_1,
                    BB = "group1")
d2 <- pf2(H = new_H)
d2

# comparing get_policy_functions() and get_policy() when
# used on an object of class "policy_eval":
new_H <- get_history(pd, stage = 2, full_history = TRUE)$H
new_H$L <- new_H$L_2
new_H$C <- new_H$C_2
all.equal(
 unname(pf2(H = new_H)),
 get_policy(pe)(pd)[stage == 2]$d
)
}
